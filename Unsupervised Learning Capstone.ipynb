{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import ensemble\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background \n",
    "\n",
    "First, pick a set of texts. This can be either a series of novels, chapters, or articles. Anything you'd like. It just has to have multiple entries of varying characteristics. At least 100 should be good. There should also be at least 10 different authors, but try to keep the texts related (either all on the same topic of from the same branch of literature - something to make classification a bit more difficult than obviously different subjects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets look at what text I can use. \n",
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick 10 different books with 10 different authors.\n",
    "austen = gutenberg.raw('austen-emma.txt')\n",
    "bible = gutenberg.raw('bible-kjv.txt')\n",
    "blake = gutenberg.raw('blake-poems.txt')\n",
    "bryant = gutenberg.raw('bryant-stories.txt')\n",
    "buster = gutenberg.raw('burgess-busterbrown.txt')\n",
    "chesterton = gutenberg.raw('chesterton-thursday.txt')\n",
    "edgeworth = gutenberg.raw('edgeworth-parents.txt')\n",
    "milton = gutenberg.raw('milton-paradise.txt')\n",
    "shakes = gutenberg.raw('shakespeare-caesar.txt')\n",
    "whitman = gutenberg.raw('whitman-leaves.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to clean text.\n",
    "def text_cleaner(text):\n",
    "    \n",
    "    # Visual inspection shows spaCy does not recognize the double dash '--'.\n",
    "    # Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    \n",
    "    # Get rid of headings in square brackets.\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    \n",
    "    # Get rid of chapter titles.\n",
    "    text = re.sub(r'Chapter \\d+','',text)\n",
    "    text = re.sub(r'CHAPTER \\d+', '', text)\n",
    "    text = re.sub(\"\\\\n\\\\n.*?\\\\n\\\\n\", '', text)\n",
    "  \n",
    "    # Get rid of extra whitespace.\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean documents\n",
    "austen = text_cleaner(austen)\n",
    "# Bible was over the limit for nlp so I had to limit it.\n",
    "bible = text_cleaner(bible)[:99990]\n",
    "blake = text_cleaner(blake)\n",
    "bryant = text_cleaner(bryant)\n",
    "buster = text_cleaner(buster)\n",
    "chesterton = text_cleaner(chesterton)\n",
    "edgeworth = text_cleaner(edgeworth)\n",
    "milton = text_cleaner(milton)\n",
    "shakes = text_cleaner(shakes)\n",
    "whitman = text_cleaner(whitman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run spaCy and analyze the documents\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "\n",
    "austen_doc = nlp(austen)\n",
    "bible_doc = nlp(bible)\n",
    "blake_doc = nlp(blake)\n",
    "bryant_doc = nlp(bryant)\n",
    "buster_doc = nlp(buster)\n",
    "chesterton_doc = nlp(chesterton)\n",
    "edgeworth_doc = nlp(edgeworth)\n",
    "milton_doc = nlp(milton)\n",
    "shakes_doc = nlp(shakes)\n",
    "whitman_doc = nlp(whitman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group into sentences\n",
    "austen_sents = [[sent, 'austen'] for sent in austen_doc.sents]\n",
    "bible_sents = [[sent, 'bible'] for sent in bible_doc.sents]\n",
    "bryant_sents = [[sent, 'bryant'] for sent in bryant_doc.sents]\n",
    "buster_sents = [[sent, 'buster'] for sent in buster_doc.sents]\n",
    "chesterton_sents = [[sent, 'chesterton'] for sent in chesterton_doc.sents]\n",
    "edgeworth_sents = [[sent, 'edgeworth'] for sent in edgeworth_doc.sents]\n",
    "milton_sents = [[sent, 'milton'] for sent in milton_doc.sents]\n",
    "shakes_sents = [[sent, 'shakes'] for sent in shakes_doc.sents]\n",
    "whitman_sents = [[sent, 'whitman'] for sent in whitman_doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the sentences from the 10 novels into one data frame.\n",
    "sentences = pd.DataFrame(austen_sents + bible_sents + bryant_sents +\n",
    "                        buster_sents + chesterton_sents + edgeworth_sents +\n",
    "                        milton_sents + shakes_sents + whitman_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(CHAPTER, I, Emma, Woodhouse, ,, handsome, ,, ...</td>\n",
       "      <td>austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(She, was, the, youngest, of, the, two, daught...</td>\n",
       "      <td>austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Her, mother, had, died, too, long, ago, for, ...</td>\n",
       "      <td>austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Sixteen, years, had, Miss, Taylor, been, in, ...</td>\n",
       "      <td>austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Between, _, them)</td>\n",
       "      <td>austen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  author\n",
       "0  (CHAPTER, I, Emma, Woodhouse, ,, handsome, ,, ...  austen\n",
       "1  (She, was, the, youngest, of, the, two, daught...  austen\n",
       "2  (Her, mother, had, died, too, long, ago, for, ...  austen\n",
       "3  (Sixteen, years, had, Miss, Taylor, been, in, ...  austen\n",
       "4                                 (Between, _, them)  austen"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a title to the columns so we know what we are looking at.\n",
    "sentences.columns = ['text', 'author']\n",
    "\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add another column turning the author into numbers.\n",
    "sentences['num_author'] = sentences['author'].map({'austen':1, 'bible':2, 'blake':3,\n",
    "                                                  'bryant':4, 'buster':5, 'chesterton':6,\n",
    "                                                  'edgeworth':7, 'milton':8, 'shakes':9,\n",
    "                                                  'whitman':10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>num_author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(CHAPTER, I, Emma, Woodhouse, ,, handsome, ,, ...</td>\n",
       "      <td>austen</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(She, was, the, youngest, of, the, two, daught...</td>\n",
       "      <td>austen</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Her, mother, had, died, too, long, ago, for, ...</td>\n",
       "      <td>austen</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Sixteen, years, had, Miss, Taylor, been, in, ...</td>\n",
       "      <td>austen</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Between, _, them)</td>\n",
       "      <td>austen</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  author  num_author\n",
       "0  (CHAPTER, I, Emma, Woodhouse, ,, handsome, ,, ...  austen           1\n",
       "1  (She, was, the, youngest, of, the, two, daught...  austen           1\n",
       "2  (Her, mother, had, died, too, long, ago, for, ...  austen           1\n",
       "3  (Sixteen, years, had, Miss, Taylor, been, in, ...  austen           1\n",
       "4                                 (Between, _, them)  austen           1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37215, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the size of the data. \n",
    "sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text          object\n",
       "author        object\n",
       "num_author     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I tried vectorizing before but the program wouldn't run.\n",
    "# The error said that it wanted string type data so I checked my data type.\n",
    "sentences.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Originally, sentences was an object type, need to convert it to str type.\n",
    "sentences['text'] = sentences['text'].astype('str') \n",
    "sentences['author'] = sentences['author'].astype('str') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a training and testing data set. \n",
    "X = sentences['text']\n",
    "y = sentences['num_author']\n",
    "\n",
    "# Splitting into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Generation\n",
    "\n",
    "## Tf-idf Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 10131\n",
      "Original sentence: Oh no!\n",
      "Tf_idf vector: {'Shake': 0.533733191309191, 'Corkscrew': 0.4179025907423565, 'fellow': 0.36685343522395064, 'honest': 0.38448314112456033, 'hands': 0.31609340947531134, 'glad': 0.3499317762019466, 'said': 0.188971651026113}\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.5,\n",
    "                            min_df=4,\n",
    "                            stop_words='english',\n",
    "                            lowercase=False,\n",
    "                            use_idf=True,\n",
    "                            norm=u'l2',\n",
    "                            smooth_idf=True)\n",
    "# Applying the vectorizer\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "print('Number of features: {}'.format(X_tfidf.get_shape()[1]))\n",
    "\n",
    "# Splitting into train and test sets\n",
    "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Reshape vectorizer to readable content\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "\n",
    "# Number of paragraphs\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "\n",
    "# A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "\n",
    "# List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "# For each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i, j]\n",
    "\n",
    "# Keep in mind that the log base 2 of 1 is 0, \n",
    "# so a tf-idf score of 0 indicates that the word was present once in that sentence.\n",
    "print('Original sentence:', X_train[100])\n",
    "print('Tf_idf vector:', tfidf_bypara[100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data.\n",
    "from sklearn.preprocessing import normalize\n",
    "X_norm = normalize(X_train_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP - spaCy\n",
    "\n",
    "Rerun NLP in order to tokenize each sentence to be able to extract information about parts of speech to add as features in our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating spaCy\n",
    "nlp = spacy.load('en')\n",
    "X_train_words = []\n",
    "\n",
    "for row in X_train:\n",
    "    # Processing each row for tokens\n",
    "    row_doc = nlp(row)\n",
    "    # Calculating length of each sentence\n",
    "    sent_len = len(row_doc) \n",
    "    # Initializing counts of different parts of speech\n",
    "    advs = 0\n",
    "    verb = 0\n",
    "    noun = 0\n",
    "    adj = 0\n",
    "    for token in row_doc:\n",
    "        # Identifying each part of speech and adding to counts\n",
    "        if token.pos_ == 'ADV':\n",
    "            advs +=1\n",
    "        elif token.pos_ == 'VERB':\n",
    "            verb +=1\n",
    "        elif token.pos_ == 'NOUN':\n",
    "            noun +=1\n",
    "        elif token.pos_ == 'ADJ':\n",
    "            adj +=1\n",
    "    # Creating a list of all features for each sentence\n",
    "    X_train_words.append([row_doc, advs, verb, noun, adj, sent_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to label each of these new features, I need to re index the y_train data\n",
    "y_train_new = y_train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BoW</th>\n",
       "      <th>Adv</th>\n",
       "      <th>Verb</th>\n",
       "      <th>Noun</th>\n",
       "      <th>Adj</th>\n",
       "      <th>Sent_len</th>\n",
       "      <th>author</th>\n",
       "      <th>num_author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(There, had, been, no, real, affection, either...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>austen</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(The, first, to, speak, was, Gogol, ,, the, ir...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>chesterton</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(I, swear, I, will, never, henceforth, have, t...</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>whitman</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(`, If, _, Miss, _, _, Taylor, _, undertakes, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>austen</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(If, we, go, anywhere, we, 'll, go, together, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>whitman</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 BoW  Adv  Verb  Noun  Adj  \\\n",
       "0  (There, had, been, no, real, affection, either...    1     2     3    2   \n",
       "1  (The, first, to, speak, was, Gogol, ,, the, ir...    0     4     2    3   \n",
       "2  (I, swear, I, will, never, henceforth, have, t...    3     9     2    5   \n",
       "3  (`, If, _, Miss, _, _, Taylor, _, undertakes, ...    1     5     5    0   \n",
       "4  (If, we, go, anywhere, we, 'll, go, together, ...    2     5     1    0   \n",
       "\n",
       "   Sent_len      author  num_author  \n",
       "0        13      austen           1  \n",
       "1        18  chesterton           6  \n",
       "2        32     whitman          10  \n",
       "3        25      austen           1  \n",
       "4        13     whitman          10  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put the new features together with their respective authors.\n",
    "# Create a data frame for the features first.\n",
    "txt_bow = pd.DataFrame(data=X_train_words, columns=['BoW', 'Adv', 'Verb', 'Noun',\n",
    "                                                   'Adj', 'Sent_len'])\n",
    "# Add the author data into the data frame.\n",
    "txt_bow = pd.concat([txt_bow, y_train_new], ignore_index=False, axis=1)\n",
    "txt_bow.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BoW</th>\n",
       "      <th>Adv</th>\n",
       "      <th>Verb</th>\n",
       "      <th>Noun</th>\n",
       "      <th>Adj</th>\n",
       "      <th>Sent_len</th>\n",
       "      <th>author</th>\n",
       "      <th>num_author</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>...</th>\n",
       "      <th>10121</th>\n",
       "      <th>10122</th>\n",
       "      <th>10123</th>\n",
       "      <th>10124</th>\n",
       "      <th>10125</th>\n",
       "      <th>10126</th>\n",
       "      <th>10127</th>\n",
       "      <th>10128</th>\n",
       "      <th>10129</th>\n",
       "      <th>10130</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(There, had, been, no, real, affection, either...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>austen</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(The, first, to, speak, was, Gogol, ,, the, ir...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>chesterton</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(I, swear, I, will, never, henceforth, have, t...</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>whitman</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(`, If, _, Miss, _, _, Taylor, _, undertakes, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>austen</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(If, we, go, anywhere, we, 'll, go, together, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>whitman</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 10139 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 BoW  Adv  Verb  Noun  Adj  \\\n",
       "0  (There, had, been, no, real, affection, either...    1     2     3    2   \n",
       "1  (The, first, to, speak, was, Gogol, ,, the, ir...    0     4     2    3   \n",
       "2  (I, swear, I, will, never, henceforth, have, t...    3     9     2    5   \n",
       "3  (`, If, _, Miss, _, _, Taylor, _, undertakes, ...    1     5     5    0   \n",
       "4  (If, we, go, anywhere, we, 'll, go, together, ...    2     5     1    0   \n",
       "\n",
       "   Sent_len      author  num_author    0    1  ...    10121  10122  10123  \\\n",
       "0        13      austen           1  0.0  0.0  ...      0.0    0.0    0.0   \n",
       "1        18  chesterton           6  0.0  0.0  ...      0.0    0.0    0.0   \n",
       "2        32     whitman          10  0.0  0.0  ...      0.0    0.0    0.0   \n",
       "3        25      austen           1  0.0  0.0  ...      0.0    0.0    0.0   \n",
       "4        13     whitman          10  0.0  0.0  ...      0.0    0.0    0.0   \n",
       "\n",
       "   10124  10125  10126  10127  10128  10129  10130  \n",
       "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "1    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "2    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "3    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "4    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "\n",
       "[5 rows x 10139 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Okay so I have the boW as features,\n",
    "# Now I want to obtain the tf-idf features too. \n",
    "X_norm_df = pd.DataFrame(data=X_norm.toarray())\n",
    "txt_tfidf_bow = pd.concat([txt_bow, X_norm_df], ignore_index=False, axis=1)\n",
    "txt_tfidf_bow.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training dataframe shape: (27911, 10139)\n"
     ]
    }
   ],
   "source": [
    "print('Final training dataframe shape:', txt_tfidf_bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying features and labels to choose from\n",
    "features = txt_tfidf_bow.drop(['BoW', 'author', 'num_author'], axis=1)\n",
    "y2_train = txt_tfidf_bow['num_author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theres too many features at the moment, so I will limit it to top 150.\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# Instantiating and fitting the 150 best features\n",
    "kbest = SelectKBest(chi2, k=150)\n",
    "X2_train = kbest.fit_transform(features, y2_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "The point of using clusters is to group the paragraphs together to see if the clusters group accordingly to their author. There will be several different clustering techniques, the first being\n",
    "\n",
    "## K-Means Clustering\n",
    "\n",
    "K-means clustering is an iterative algorithm that seeks to cluster based on minimizing the inertia (cost function) or the sum of squared differences between the mean of the cluster and the data points of the cluster. \n",
    "\n",
    "Normally, the data would have to be normalized before using K-Means so that the distance would be accurate but that has already been done up top. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_author</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2164</td>\n",
       "      <td>666</td>\n",
       "      <td>20</td>\n",
       "      <td>1896</td>\n",
       "      <td>0</td>\n",
       "      <td>330</td>\n",
       "      <td>105</td>\n",
       "      <td>0</td>\n",
       "      <td>1093</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>489</td>\n",
       "      <td>109</td>\n",
       "      <td>0</td>\n",
       "      <td>218</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>202</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>610</td>\n",
       "      <td>183</td>\n",
       "      <td>1</td>\n",
       "      <td>720</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>509</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>277</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>275</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>522</td>\n",
       "      <td>235</td>\n",
       "      <td>0</td>\n",
       "      <td>866</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>622</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2842</td>\n",
       "      <td>797</td>\n",
       "      <td>14</td>\n",
       "      <td>2083</td>\n",
       "      <td>0</td>\n",
       "      <td>332</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>1523</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>454</td>\n",
       "      <td>371</td>\n",
       "      <td>53</td>\n",
       "      <td>453</td>\n",
       "      <td>0</td>\n",
       "      <td>237</td>\n",
       "      <td>139</td>\n",
       "      <td>0</td>\n",
       "      <td>374</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>889</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>331</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>143</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1656</td>\n",
       "      <td>478</td>\n",
       "      <td>72</td>\n",
       "      <td>959</td>\n",
       "      <td>4</td>\n",
       "      <td>259</td>\n",
       "      <td>154</td>\n",
       "      <td>1</td>\n",
       "      <td>637</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0          0    1   2     3  4    5    6  7     8   9\n",
       "num_author                                               \n",
       "1           2164  666  20  1896  0  330  105  0  1093   0\n",
       "2            489  109   0   218  0   36    1  0   202   0\n",
       "4            610  183   1   720  0   58    2  0   509   0\n",
       "5            277   61   0   275  0   16    2  0   133   0\n",
       "6            522  235   0   866  0   60    7  0   622   0\n",
       "7           2842  797  14  2083  0  332   88  0  1523   1\n",
       "8            454  371  53   453  0  237  139  0   374   1\n",
       "9            889   52   0   331  0    9    2  0   143   0\n",
       "10          1656  478  72   959  4  259  154  1   637  15"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# I will use 10 clusters because I have 10 authors.\n",
    "# Initialize the model. \n",
    "kmeans = KMeans(n_clusters=10, init='k-means++', random_state=42, n_init=20)\n",
    "\n",
    "# Fit and predict the model. \n",
    "y_pred = kmeans.fit_predict(X2_train)\n",
    "\n",
    "pd.crosstab(y2_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Rand Score: 0.006425331\n",
      "Silhouette Score: 0.4459991\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.metrics import silhouette_score\n",
    "# Evaluate the performance of the clusters\n",
    "\n",
    "print('Adjusted Rand Score: {:0.7}'.format(adjusted_rand_score(y2_train, y_pred)))\n",
    "print('Silhouette Score: {:0.7}'.format(silhouette_score(X2_train, y_pred, metric='euclidean')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjusted Rand Index is a function that measures the similarity of two assignments, ignoring permutations and with chance normalization. The score was negative which means that the similarity might as well be random. ARI has an accurate ground truth in the y-pred so it signals that the similarity isn't there.\n",
    "\n",
    "Silhouette Coefficient is the mean distance between a sample and all other points in the same class (a), the mean distance between a sample and all other points in the next nearest cluster (b), divided by whichever of the two values is highest. Scores around 0 indicate overlapping clusters. Score is higher when clusters are dense and well separated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a19456b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "# Use the elbow method to see what is the optimal amount of clusters.\n",
    "\n",
    "# k means determine k\n",
    "distortions = []\n",
    "K = range(1,20)\n",
    "for k in K:\n",
    "    kmeanModel = KMeans(n_clusters=k).fit(X2_train)\n",
    "    kmeanModel.fit(X2_train)\n",
    "    distortions.append(sum(np.min(cdist(X2_train, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / X2_train.shape[0])\n",
    "\n",
    "# Plot the elbow\n",
    "sns.set_style('darkgrid')\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elbow is actually around 6 clusters but I have to use 10 in order to see if the models can label the 10 authors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Batch K Means \n",
    "\n",
    "Mini Batch won't really change much since it is only used when PCA isn't run to reduce dimensionality and search for clusters in the reduced data. Mini Batch is useful if I want to keep all the data and if I have limited computational power or time. It works by randomly sampling subsets of the training data in each iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_author</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>860</td>\n",
       "      <td>518</td>\n",
       "      <td>1121</td>\n",
       "      <td>42</td>\n",
       "      <td>170</td>\n",
       "      <td>1513</td>\n",
       "      <td>320</td>\n",
       "      <td>0</td>\n",
       "      <td>672</td>\n",
       "      <td>1058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>145</td>\n",
       "      <td>85</td>\n",
       "      <td>443</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>83</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>131</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>412</td>\n",
       "      <td>128</td>\n",
       "      <td>324</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>416</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>296</td>\n",
       "      <td>427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>109</td>\n",
       "      <td>47</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>224</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>486</td>\n",
       "      <td>175</td>\n",
       "      <td>221</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>464</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>366</td>\n",
       "      <td>513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1162</td>\n",
       "      <td>610</td>\n",
       "      <td>1815</td>\n",
       "      <td>27</td>\n",
       "      <td>150</td>\n",
       "      <td>1491</td>\n",
       "      <td>350</td>\n",
       "      <td>1</td>\n",
       "      <td>915</td>\n",
       "      <td>1159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>249</td>\n",
       "      <td>284</td>\n",
       "      <td>215</td>\n",
       "      <td>89</td>\n",
       "      <td>183</td>\n",
       "      <td>309</td>\n",
       "      <td>211</td>\n",
       "      <td>1</td>\n",
       "      <td>247</td>\n",
       "      <td>294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>136</td>\n",
       "      <td>42</td>\n",
       "      <td>613</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>354</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>456</td>\n",
       "      <td>361</td>\n",
       "      <td>1094</td>\n",
       "      <td>127</td>\n",
       "      <td>169</td>\n",
       "      <td>798</td>\n",
       "      <td>245</td>\n",
       "      <td>20</td>\n",
       "      <td>406</td>\n",
       "      <td>559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0          0    1     2    3    4     5    6   7    8     9\n",
       "num_author                                                     \n",
       "1            860  518  1121   42  170  1513  320   0  672  1058\n",
       "2            145   85   443    0    8    83   36   0  131   124\n",
       "4            412  128   324    1    9   416   70   0  296   427\n",
       "5            109   47   120    0    6   224   19   0   77   162\n",
       "6            486  175   221    1   16   464   70   0  366   513\n",
       "7           1162  610  1815   27  150  1491  350   1  915  1159\n",
       "8            249  284   215   89  183   309  211   1  247   294\n",
       "9            136   42   613    0    5   354    8   0   76   192\n",
       "10           456  361  1094  127  169   798  245  20  406   559"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "# Initialize the model. \n",
    "minikmeans = MiniBatchKMeans(n_clusters=10, init='k-means++', random_state=42, init_size=1000, batch_size=1000)\n",
    "\n",
    "# Predict and fit the model. \n",
    "y_pred2 = minikmeans.fit_predict(X2_train)\n",
    "\n",
    "pd.crosstab(y2_train, y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Rand Score: 0.007552872\n",
      "Silhouette Score: 0.3612096\n"
     ]
    }
   ],
   "source": [
    "print('Adjusted Rand Score: {:0.7}'.format(adjusted_rand_score(y2_train, y_pred2)))\n",
    "print('Silhouette Score: {:0.7}'.format(silhouette_score(X2_train, y_pred2, metric='euclidean')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This did slightly worse than k-means but that is to be expected since this is just another rendition of the original k-means model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral Clustering\n",
    "\n",
    "Spectral Clustering is based on quantifying similarity between data points. Spectral clustering takes many different measures of similiarity. The two most common ones are nearest neighbor and the Gaussian kernel of the Euclidean distance. I will put in 10 clusters and the 10 eigenvectors with the 10 largest eigenvalues are extracted and the data is converted to the new 10 dimensional space. This will take a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import SpectralClustering\n",
    "# # Pick the number of clusters.\n",
    "# n_clusters= 10\n",
    "\n",
    "# # Initialize the model.\n",
    "# sc = SpectralClustering(n_clusters=n_clusters)\n",
    "\n",
    "# # Fit and predict the model.\n",
    "# y_pred3 = sc.fit_predict(X2_train)\n",
    "\n",
    "# pd.crosstab(y2_train, y_pred3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would've shown the results the spectral clustering model but it took 11 hours and the program was still running. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n",
      "\n",
      "               Adv         Verb         Noun          Adj     Sent_len  \\\n",
      "count  9900.000000  9900.000000  9900.000000  9900.000000  9900.000000   \n",
      "mean      0.446263     1.065859     0.799899     0.445758     6.232323   \n",
      "std       0.700281     1.008854     0.886907     0.695030     3.159524   \n",
      "min       0.000000     0.000000     0.000000     0.000000     1.000000   \n",
      "25%       0.000000     0.000000     0.000000     0.000000     4.000000   \n",
      "50%       0.000000     1.000000     1.000000     0.000000     6.000000   \n",
      "75%       1.000000     2.000000     1.000000     1.000000     9.000000   \n",
      "max       5.000000     5.000000     6.000000     5.000000    12.000000   \n",
      "\n",
      "                 0            1            2            3            4  \\\n",
      "count  9900.000000  9900.000000  9900.000000  9900.000000  9900.000000   \n",
      "mean      0.002687     0.002360     0.002514     0.001979     0.002625   \n",
      "std       0.047367     0.044323     0.044919     0.040770     0.046339   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
      "\n",
      "              ...                10122   10123   10124        10125   10126  \\\n",
      "count         ...          9900.000000  9900.0  9900.0  9900.000000  9900.0   \n",
      "mean          ...             0.000059     0.0     0.0     0.000192     0.0   \n",
      "std           ...             0.005838     0.0     0.0     0.009593     0.0   \n",
      "min           ...             0.000000     0.0     0.0     0.000000     0.0   \n",
      "25%           ...             0.000000     0.0     0.0     0.000000     0.0   \n",
      "50%           ...             0.000000     0.0     0.0     0.000000     0.0   \n",
      "75%           ...             0.000000     0.0     0.0     0.000000     0.0   \n",
      "max           ...             0.580862     0.0     0.0     0.534573     0.0   \n",
      "\n",
      "             10127   10128   10129   10130  cluster_assignment  \n",
      "count  9900.000000  9900.0  9900.0  9900.0              9900.0  \n",
      "mean      0.000062     0.0     0.0     0.0                 0.0  \n",
      "std       0.006158     0.0     0.0     0.0                 0.0  \n",
      "min       0.000000     0.0     0.0     0.0                 0.0  \n",
      "25%       0.000000     0.0     0.0     0.0                 0.0  \n",
      "50%       0.000000     0.0     0.0     0.0                 0.0  \n",
      "75%       0.000000     0.0     0.0     0.0                 0.0  \n",
      "max       0.612711     0.0     0.0     0.0                 0.0  \n",
      "\n",
      "[8 rows x 10137 columns]\n",
      "1\n",
      "\n",
      "\n",
      "               Adv         Verb         Noun          Adj     Sent_len  \\\n",
      "count  2954.000000  2954.000000  2954.000000  2954.000000  2954.000000   \n",
      "mean      2.604604     6.899458     6.597833     4.100880    42.264726   \n",
      "std       1.864362     2.431391     2.609747     2.173941     4.831849   \n",
      "min       0.000000     0.000000     0.000000     0.000000    35.000000   \n",
      "25%       1.000000     5.000000     5.000000     3.000000    38.000000   \n",
      "50%       2.000000     7.000000     6.000000     4.000000    42.000000   \n",
      "75%       4.000000     8.000000     8.000000     5.000000    46.000000   \n",
      "max      11.000000    17.000000    17.000000    14.000000    53.000000   \n",
      "\n",
      "                 0            1            2            3            4  \\\n",
      "count  2954.000000  2954.000000  2954.000000  2954.000000  2954.000000   \n",
      "mean      0.000286     0.000134     0.000321     0.000162     0.000148   \n",
      "std       0.009206     0.005162     0.010915     0.006229     0.005735   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "max       0.370961     0.213084     0.503428     0.242421     0.243235   \n",
      "\n",
      "              ...                10122        10123   10124        10125  \\\n",
      "count         ...          2954.000000  2954.000000  2954.0  2954.000000   \n",
      "mean          ...             0.000406     0.000116     0.0     0.001159   \n",
      "std           ...             0.009964     0.006293     0.0     0.018221   \n",
      "min           ...             0.000000     0.000000     0.0     0.000000   \n",
      "25%           ...             0.000000     0.000000     0.0     0.000000   \n",
      "50%           ...             0.000000     0.000000     0.0     0.000000   \n",
      "75%           ...             0.000000     0.000000     0.0     0.000000   \n",
      "max           ...             0.295864     0.342051     0.0     0.494852   \n",
      "\n",
      "             10126        10127        10128        10129   10130  \\\n",
      "count  2954.000000  2954.000000  2954.000000  2954.000000  2954.0   \n",
      "mean      0.000307     0.000113     0.000387     0.000108     0.0   \n",
      "std       0.009651     0.006155     0.010539     0.005858     0.0   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.0   \n",
      "25%       0.000000     0.000000     0.000000     0.000000     0.0   \n",
      "50%       0.000000     0.000000     0.000000     0.000000     0.0   \n",
      "75%       0.000000     0.000000     0.000000     0.000000     0.0   \n",
      "max       0.326820     0.334538     0.322706     0.318409     0.0   \n",
      "\n",
      "       cluster_assignment  \n",
      "count              2954.0  \n",
      "mean                  1.0  \n",
      "std                   0.0  \n",
      "min                   1.0  \n",
      "25%                   1.0  \n",
      "50%                   1.0  \n",
      "75%                   1.0  \n",
      "max                   1.0  \n",
      "\n",
      "[8 rows x 10137 columns]\n",
      "2\n",
      "\n",
      "\n",
      "              Adv        Verb        Noun         Adj    Sent_len      0  \\\n",
      "count  500.000000  500.000000  500.000000  500.000000  500.000000  500.0   \n",
      "mean     5.354000   13.384000   16.582000    9.888000   93.868000    0.0   \n",
      "std      3.084755    4.744847    5.767304    3.937316   11.728328    0.0   \n",
      "min      0.000000    1.000000    3.000000    0.000000   78.000000    0.0   \n",
      "25%      3.000000   10.000000   12.000000    7.000000   84.000000    0.0   \n",
      "50%      5.000000   13.000000   16.000000   10.000000   91.500000    0.0   \n",
      "75%      7.000000   17.000000   20.000000   12.000000  103.000000    0.0   \n",
      "max     18.000000   28.000000   37.000000   24.000000  122.000000    0.0   \n",
      "\n",
      "           1      2           3      4         ...               10122  \\\n",
      "count  500.0  500.0  500.000000  500.0         ...          500.000000   \n",
      "mean     0.0    0.0    0.000273    0.0         ...            0.000381   \n",
      "std      0.0    0.0    0.006105    0.0         ...            0.008524   \n",
      "min      0.0    0.0    0.000000    0.0         ...            0.000000   \n",
      "25%      0.0    0.0    0.000000    0.0         ...            0.000000   \n",
      "50%      0.0    0.0    0.000000    0.0         ...            0.000000   \n",
      "75%      0.0    0.0    0.000000    0.0         ...            0.000000   \n",
      "max      0.0    0.0    0.136510    0.0         ...            0.190603   \n",
      "\n",
      "            10123  10124       10125  10126  10127       10128       10129  \\\n",
      "count  500.000000  500.0  500.000000  500.0  500.0  500.000000  500.000000   \n",
      "mean     0.000537    0.0    0.000984    0.0    0.0    0.000310    0.000422   \n",
      "std      0.011998    0.0    0.012754    0.0    0.0    0.006939    0.009442   \n",
      "min      0.000000    0.0    0.000000    0.0    0.0    0.000000    0.000000   \n",
      "25%      0.000000    0.0    0.000000    0.0    0.0    0.000000    0.000000   \n",
      "50%      0.000000    0.0    0.000000    0.0    0.0    0.000000    0.000000   \n",
      "75%      0.000000    0.0    0.000000    0.0    0.0    0.000000    0.000000   \n",
      "max      0.268278    0.0    0.177021    0.0    0.0    0.155163    0.211131   \n",
      "\n",
      "       10130  cluster_assignment  \n",
      "count  500.0               500.0  \n",
      "mean     0.0                 2.0  \n",
      "std      0.0                 0.0  \n",
      "min      0.0                 2.0  \n",
      "25%      0.0                 2.0  \n",
      "50%      0.0                 2.0  \n",
      "75%      0.0                 2.0  \n",
      "max      0.0                 2.0  \n",
      "\n",
      "[8 rows x 10137 columns]\n",
      "3\n",
      "\n",
      "\n",
      "               Adv         Verb         Noun          Adj     Sent_len  \\\n",
      "count  5241.000000  5241.000000  5241.000000  5241.000000  5241.000000   \n",
      "mean      1.807289     4.751765     4.058767     2.492463    27.976913   \n",
      "std       1.474553     1.866542     1.931355     1.635015     3.600949   \n",
      "min       0.000000     0.000000     0.000000     0.000000    22.000000   \n",
      "25%       1.000000     3.000000     3.000000     1.000000    25.000000   \n",
      "50%       2.000000     5.000000     4.000000     2.000000    28.000000   \n",
      "75%       3.000000     6.000000     5.000000     3.000000    31.000000   \n",
      "max       9.000000    13.000000    13.000000    10.000000    36.000000   \n",
      "\n",
      "                 0            1            2            3            4  \\\n",
      "count  5241.000000  5241.000000  5241.000000  5241.000000  5241.000000   \n",
      "mean      0.000292     0.000203     0.000225     0.000330     0.000097   \n",
      "std       0.013124     0.007596     0.008151     0.010727     0.004978   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "max       0.747744     0.375675     0.300791     0.385050     0.277234   \n",
      "\n",
      "              ...                10122        10123        10124        10125  \\\n",
      "count         ...          5241.000000  5241.000000  5241.000000  5241.000000   \n",
      "mean          ...             0.000221     0.000330     0.000088     0.000416   \n",
      "std           ...             0.008033     0.010758     0.006381     0.011424   \n",
      "min           ...             0.000000     0.000000     0.000000     0.000000   \n",
      "25%           ...             0.000000     0.000000     0.000000     0.000000   \n",
      "50%           ...             0.000000     0.000000     0.000000     0.000000   \n",
      "75%           ...             0.000000     0.000000     0.000000     0.000000   \n",
      "max           ...             0.324253     0.383911     0.461926     0.373140   \n",
      "\n",
      "        10126        10127        10128   10129       10130  \\\n",
      "count  5241.0  5241.000000  5241.000000  5241.0  5241.00000   \n",
      "mean      0.0     0.000144     0.000149     0.0     0.00007   \n",
      "std       0.0     0.007403     0.007773     0.0     0.00508   \n",
      "min       0.0     0.000000     0.000000     0.0     0.00000   \n",
      "25%       0.0     0.000000     0.000000     0.0     0.00000   \n",
      "50%       0.0     0.000000     0.000000     0.0     0.00000   \n",
      "75%       0.0     0.000000     0.000000     0.0     0.00000   \n",
      "max       0.0     0.420342     0.464733     0.0     0.36780   \n",
      "\n",
      "       cluster_assignment  \n",
      "count              5241.0  \n",
      "mean                  3.0  \n",
      "std                   0.0  \n",
      "min                   3.0  \n",
      "25%                   3.0  \n",
      "50%                   3.0  \n",
      "75%                   3.0  \n",
      "max                   3.0  \n",
      "\n",
      "[8 rows x 10137 columns]\n",
      "4\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Adv       Verb        Noun        Adj    Sent_len     0     1  \\\n",
      "count  16.000000  16.000000   16.000000  16.000000   16.000000  16.0  16.0   \n",
      "mean   11.687500  31.687500   62.812500  26.625000  268.562500   0.0   0.0   \n",
      "std     6.279265   9.257204   23.235659   7.013083   53.346626   0.0   0.0   \n",
      "min     1.000000  18.000000   37.000000  18.000000  216.000000   0.0   0.0   \n",
      "25%     7.750000  25.000000   45.000000  21.500000  237.500000   0.0   0.0   \n",
      "50%    11.000000  32.500000   61.000000  24.500000  246.000000   0.0   0.0   \n",
      "75%    14.500000  36.250000   67.750000  32.750000  281.250000   0.0   0.0   \n",
      "max    28.000000  48.000000  114.000000  38.000000  390.000000   0.0   0.0   \n",
      "\n",
      "          2     3     4         ...              10122  10123  10124  \\\n",
      "count  16.0  16.0  16.0         ...          16.000000   16.0   16.0   \n",
      "mean    0.0   0.0   0.0         ...           0.005012    0.0    0.0   \n",
      "std     0.0   0.0   0.0         ...           0.020049    0.0    0.0   \n",
      "min     0.0   0.0   0.0         ...           0.000000    0.0    0.0   \n",
      "25%     0.0   0.0   0.0         ...           0.000000    0.0    0.0   \n",
      "50%     0.0   0.0   0.0         ...           0.000000    0.0    0.0   \n",
      "75%     0.0   0.0   0.0         ...           0.000000    0.0    0.0   \n",
      "max     0.0   0.0   0.0         ...           0.080197    0.0    0.0   \n",
      "\n",
      "           10125  10126  10127  10128  10129  10130  cluster_assignment  \n",
      "count  16.000000   16.0   16.0   16.0   16.0   16.0                16.0  \n",
      "mean    0.004236    0.0    0.0    0.0    0.0    0.0                 4.0  \n",
      "std     0.016946    0.0    0.0    0.0    0.0    0.0                 0.0  \n",
      "min     0.000000    0.0    0.0    0.0    0.0    0.0                 4.0  \n",
      "25%     0.000000    0.0    0.0    0.0    0.0    0.0                 4.0  \n",
      "50%     0.000000    0.0    0.0    0.0    0.0    0.0                 4.0  \n",
      "75%     0.000000    0.0    0.0    0.0    0.0    0.0                 4.0  \n",
      "max     0.067782    0.0    0.0    0.0    0.0    0.0                 4.0  \n",
      "\n",
      "[8 rows x 10137 columns]\n",
      "5\n",
      "\n",
      "\n",
      "        Adv  Verb   Noun   Adj  Sent_len    0    1    2    3    4  \\\n",
      "count   1.0   1.0    1.0   1.0       1.0  1.0  1.0  1.0  1.0  1.0   \n",
      "mean   21.0  58.0  326.0  98.0    1075.0  0.0  0.0  0.0  0.0  0.0   \n",
      "std     NaN   NaN    NaN   NaN       NaN  NaN  NaN  NaN  NaN  NaN   \n",
      "min    21.0  58.0  326.0  98.0    1075.0  0.0  0.0  0.0  0.0  0.0   \n",
      "25%    21.0  58.0  326.0  98.0    1075.0  0.0  0.0  0.0  0.0  0.0   \n",
      "50%    21.0  58.0  326.0  98.0    1075.0  0.0  0.0  0.0  0.0  0.0   \n",
      "75%    21.0  58.0  326.0  98.0    1075.0  0.0  0.0  0.0  0.0  0.0   \n",
      "max    21.0  58.0  326.0  98.0    1075.0  0.0  0.0  0.0  0.0  0.0   \n",
      "\n",
      "              ...          10122  10123  10124  10125  10126  10127  10128  \\\n",
      "count         ...            1.0    1.0    1.0    1.0    1.0    1.0    1.0   \n",
      "mean          ...            0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "std           ...            NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
      "min           ...            0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "25%           ...            0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "50%           ...            0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "75%           ...            0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "max           ...            0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "\n",
      "       10129  10130  cluster_assignment  \n",
      "count    1.0    1.0                 1.0  \n",
      "mean     0.0    0.0                 5.0  \n",
      "std      NaN    NaN                 NaN  \n",
      "min      0.0    0.0                 5.0  \n",
      "25%      0.0    0.0                 5.0  \n",
      "50%      0.0    0.0                 5.0  \n",
      "75%      0.0    0.0                 5.0  \n",
      "max      0.0    0.0                 5.0  \n",
      "\n",
      "[8 rows x 10137 columns]\n",
      "6\n",
      "\n",
      "\n",
      "               Adv         Verb         Noun          Adj    Sent_len  \\\n",
      "count  7796.000000  7796.000000  7796.000000  7796.000000  7796.00000   \n",
      "mean      1.154182     2.942535     2.201898     1.377886    16.52450   \n",
      "std       1.120494     1.381369     1.392190     1.180815     3.12137   \n",
      "min       0.000000     0.000000     0.000000     0.000000    11.00000   \n",
      "25%       0.000000     2.000000     1.000000     0.000000    14.00000   \n",
      "50%       1.000000     3.000000     2.000000     1.000000    16.00000   \n",
      "75%       2.000000     4.000000     3.000000     2.000000    19.00000   \n",
      "max       6.000000     9.000000     9.000000     7.000000    23.00000   \n",
      "\n",
      "                 0            1            2            3            4  \\\n",
      "count  7796.000000  7796.000000  7796.000000  7796.000000  7796.000000   \n",
      "mean      0.000129     0.000239     0.000098     0.000043     0.000058   \n",
      "std       0.008350     0.008692     0.006124     0.003822     0.005085   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "max       0.635646     0.399338     0.386638     0.337430     0.448963   \n",
      "\n",
      "              ...                10122        10123        10124        10125  \\\n",
      "count         ...          7796.000000  7796.000000  7796.000000  7796.000000   \n",
      "mean          ...             0.000337     0.000051     0.000072     0.000145   \n",
      "std           ...             0.013806     0.004461     0.006386     0.007472   \n",
      "min           ...             0.000000     0.000000     0.000000     0.000000   \n",
      "25%           ...             0.000000     0.000000     0.000000     0.000000   \n",
      "50%           ...             0.000000     0.000000     0.000000     0.000000   \n",
      "75%           ...             0.000000     0.000000     0.000000     0.000000   \n",
      "max           ...             0.795040     0.393890     0.563865     0.428508   \n",
      "\n",
      "             10126        10127   10128   10129   10130  cluster_assignment  \n",
      "count  7796.000000  7796.000000  7796.0  7796.0  7796.0              7796.0  \n",
      "mean      0.000050     0.000157     0.0     0.0     0.0                 6.0  \n",
      "std       0.004439     0.009880     0.0     0.0     0.0                 0.0  \n",
      "min       0.000000     0.000000     0.0     0.0     0.0                 6.0  \n",
      "25%       0.000000     0.000000     0.0     0.0     0.0                 6.0  \n",
      "50%       0.000000     0.000000     0.0     0.0     0.0                 6.0  \n",
      "75%       0.000000     0.000000     0.0     0.0     0.0                 6.0  \n",
      "max       0.391941     0.682886     0.0     0.0     0.0                 6.0  \n",
      "\n",
      "[8 rows x 10137 columns]\n",
      "7\n",
      "\n",
      "\n",
      "               Adv         Verb         Noun          Adj     Sent_len  \\\n",
      "count  1338.000000  1338.000000  1338.000000  1338.000000  1338.000000   \n",
      "mean      3.805680     9.784006    10.316143     6.165172    62.323617   \n",
      "std       2.296542     3.421366     3.746778     2.816568     7.230000   \n",
      "min       0.000000     1.000000     0.000000     0.000000    52.000000   \n",
      "25%       2.000000     8.000000     8.000000     4.000000    56.000000   \n",
      "50%       4.000000    10.000000    10.000000     6.000000    61.000000   \n",
      "75%       5.000000    12.000000    12.000000     8.000000    68.000000   \n",
      "max      13.000000    22.000000    24.000000    15.000000    80.000000   \n",
      "\n",
      "                 0            1            2            3            4  \\\n",
      "count  1338.000000  1338.000000  1338.000000  1338.000000  1338.000000   \n",
      "mean      0.000334     0.000143     0.000145     0.000196     0.000372   \n",
      "std       0.012202     0.005230     0.005300     0.007159     0.009719   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "max       0.446324     0.191300     0.193862     0.261870     0.286451   \n",
      "\n",
      "              ...                10122        10123   10124        10125  \\\n",
      "count         ...          1338.000000  1338.000000  1338.0  1338.000000   \n",
      "mean          ...             0.000729     0.000188     0.0     0.000627   \n",
      "std           ...             0.013363     0.006876     0.0     0.011567   \n",
      "min           ...             0.000000     0.000000     0.0     0.000000   \n",
      "25%           ...             0.000000     0.000000     0.0     0.000000   \n",
      "50%           ...             0.000000     0.000000     0.0     0.000000   \n",
      "75%           ...             0.000000     0.000000     0.0     0.000000   \n",
      "max           ...             0.279642     0.251530     0.0     0.245417   \n",
      "\n",
      "             10126        10127        10128        10129        10130  \\\n",
      "count  1338.000000  1338.000000  1338.000000  1338.000000  1338.000000   \n",
      "mean      0.000145     0.000173     0.000164     0.000310     0.000187   \n",
      "std       0.005304     0.006335     0.006015     0.008016     0.006858   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "max       0.193998     0.231734     0.220004     0.212628     0.250862   \n",
      "\n",
      "       cluster_assignment  \n",
      "count              1338.0  \n",
      "mean                  7.0  \n",
      "std                   0.0  \n",
      "min                   7.0  \n",
      "25%                   7.0  \n",
      "50%                   7.0  \n",
      "75%                   7.0  \n",
      "max                   7.0  \n",
      "\n",
      "[8 rows x 10137 columns]\n",
      "8\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Adv       Verb        Noun        Adj   Sent_len    0    1    2  \\\n",
      "count   5.00000   5.000000    5.000000   5.000000    5.00000  5.0  5.0  5.0   \n",
      "mean   15.20000  39.000000  163.600000  45.400000  551.80000  0.0  0.0  0.0   \n",
      "std     9.95992  14.142136   34.107184  18.836135   82.13221  0.0  0.0  0.0   \n",
      "min     3.00000  24.000000  116.000000  25.000000  423.00000  0.0  0.0  0.0   \n",
      "25%    11.00000  26.000000  145.000000  31.000000  519.00000  0.0  0.0  0.0   \n",
      "50%    11.00000  42.000000  168.000000  41.000000  588.00000  0.0  0.0  0.0   \n",
      "75%    25.00000  45.000000  187.000000  63.000000  606.00000  0.0  0.0  0.0   \n",
      "max    26.00000  58.000000  202.000000  67.000000  623.00000  0.0  0.0  0.0   \n",
      "\n",
      "         3    4         ...          10122  10123  10124     10125  10126  \\\n",
      "count  5.0  5.0         ...            5.0    5.0    5.0  5.000000    5.0   \n",
      "mean   0.0  0.0         ...            0.0    0.0    0.0  0.009821    0.0   \n",
      "std    0.0  0.0         ...            0.0    0.0    0.0  0.021960    0.0   \n",
      "min    0.0  0.0         ...            0.0    0.0    0.0  0.000000    0.0   \n",
      "25%    0.0  0.0         ...            0.0    0.0    0.0  0.000000    0.0   \n",
      "50%    0.0  0.0         ...            0.0    0.0    0.0  0.000000    0.0   \n",
      "75%    0.0  0.0         ...            0.0    0.0    0.0  0.000000    0.0   \n",
      "max    0.0  0.0         ...            0.0    0.0    0.0  0.049104    0.0   \n",
      "\n",
      "       10127  10128  10129  10130  cluster_assignment  \n",
      "count    5.0    5.0    5.0    5.0                 5.0  \n",
      "mean     0.0    0.0    0.0    0.0                 8.0  \n",
      "std      0.0    0.0    0.0    0.0                 0.0  \n",
      "min      0.0    0.0    0.0    0.0                 8.0  \n",
      "25%      0.0    0.0    0.0    0.0                 8.0  \n",
      "50%      0.0    0.0    0.0    0.0                 8.0  \n",
      "75%      0.0    0.0    0.0    0.0                 8.0  \n",
      "max      0.0    0.0    0.0    0.0                 8.0  \n",
      "\n",
      "[8 rows x 10137 columns]\n",
      "9\n",
      "\n",
      "\n",
      "              Adv        Verb        Noun         Adj    Sent_len           0  \\\n",
      "count  160.000000  160.000000  160.000000  160.000000  160.000000  160.000000   \n",
      "mean     7.668750   18.243750   30.125000   15.262500  147.768750    0.000704   \n",
      "std      4.673923    7.562525    9.939281    5.763131   19.260943    0.008906   \n",
      "min      0.000000    2.000000    8.000000    2.000000  121.000000    0.000000   \n",
      "25%      4.000000   12.750000   23.000000   11.000000  133.000000    0.000000   \n",
      "50%      7.000000   19.000000   29.000000   14.500000  143.000000    0.000000   \n",
      "75%     10.000000   23.000000   36.000000   19.000000  162.000000    0.000000   \n",
      "max     24.000000   38.000000   64.000000   38.000000  203.000000    0.112654   \n",
      "\n",
      "           1      2      3      4         ...               10122  10123  \\\n",
      "count  160.0  160.0  160.0  160.0         ...          160.000000  160.0   \n",
      "mean     0.0    0.0    0.0    0.0         ...            0.000866    0.0   \n",
      "std      0.0    0.0    0.0    0.0         ...            0.010960    0.0   \n",
      "min      0.0    0.0    0.0    0.0         ...            0.000000    0.0   \n",
      "25%      0.0    0.0    0.0    0.0         ...            0.000000    0.0   \n",
      "50%      0.0    0.0    0.0    0.0         ...            0.000000    0.0   \n",
      "75%      0.0    0.0    0.0    0.0         ...            0.000000    0.0   \n",
      "max      0.0    0.0    0.0    0.0         ...            0.138636    0.0   \n",
      "\n",
      "       10124       10125  10126  10127       10128  10129  10130  \\\n",
      "count  160.0  160.000000  160.0  160.0  160.000000  160.0  160.0   \n",
      "mean     0.0    0.003065    0.0    0.0    0.000915    0.0    0.0   \n",
      "std      0.0    0.019213    0.0    0.0    0.011570    0.0    0.0   \n",
      "min      0.0    0.000000    0.0    0.0    0.000000    0.0    0.0   \n",
      "25%      0.0    0.000000    0.0    0.0    0.000000    0.0    0.0   \n",
      "50%      0.0    0.000000    0.0    0.0    0.000000    0.0    0.0   \n",
      "75%      0.0    0.000000    0.0    0.0    0.000000    0.0    0.0   \n",
      "max      0.0    0.128582    0.0    0.0    0.146351    0.0    0.0   \n",
      "\n",
      "       cluster_assignment  \n",
      "count               160.0  \n",
      "mean                  9.0  \n",
      "std                   0.0  \n",
      "min                   9.0  \n",
      "25%                   9.0  \n",
      "50%                   9.0  \n",
      "75%                   9.0  \n",
      "max                   9.0  \n",
      "\n",
      "[8 rows x 10137 columns]\n"
     ]
    }
   ],
   "source": [
    "cluster_pred = KMeans(n_clusters=10, random_state=42).fit_predict(features)\n",
    "X_pred = features.copy()\n",
    "X_pred['cluster_assignment'] = cluster_pred\n",
    "\n",
    "cluster_dataframes = {}\n",
    "for n_clust in range(10):\n",
    "    cluster_dataframes[n_clust] = X_pred.loc[X_pred['cluster_assignment'] == n_clust]\n",
    "\n",
    "for name, frame in cluster_dataframes.items():\n",
    "    print(name)\n",
    "    print('\\n')\n",
    "    print(frame.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a problem. For the clusters I don't know what word is what word. To properly show the information of the clusters I will only use the part of speeches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Semantic Analysis\n",
    "\n",
    "Previously, my data set had a lot of data in it. I decided in order to remedy this by running the LSA which is a dimension reduction technique (PCA) on the tf-idf matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 99.84397238213629\n",
      "Component 0:\n",
      "text\n",
      "For a long, long time the Nightingale sang every evening to the Emperor and his court, and they liked her so much that the ladies all tried to sing like her; they used to put water in their mouths and then make little sounds like this: _glu-glu-glug_.                                                                                                                                      0.999972\n",
      "Many prudent faces amongst the Fishermen seemed to deliberate at the close of this oration, in which the arguments were brought so \"home to each man's business and bosom.\"                                                                                                                                                                                                                      0.999960\n",
      "Come Caska, you and I will yet, ere day, See Brutus at his house: three parts of him Is ours alreadie, and the man entire Vpon the next encounter, yeelds him ours Cask.                                                                                                                                                                                                                         0.999948\n",
      "He owed it to her, to risk any thing that might be involved in an unwelcome interference, rather than her welfare; to encounter any thing, rather than the remembrance of neglect in such a cause.                                                                                                                                                                                               0.999948\n",
      "A great part of the morning was spent in putting them into the ground; and, as soon as that was finished, he set out again in quest of the old woman, whom, to his great joy, he spied sitting at her corner of the street with her board before her.                                                                                                                                            0.999934\n",
      "Behold, the body includes and is the meaning, the main concern and includes and is the soul; Whoever you are, how superb and how divine is your body, or any part of it!                                                                                                                                                                                                                         0.999930\n",
      "Without extinction is Liberty, without retrograde is Equality, They live in the feelings of young men and the best women, (Not for nothing have the indomitable heads of the earth been always ready to fall for Liberty.)                                                                                                                                                                       0.999920\n",
      "After the young gentlemen walked round the carriage, Ensign Vince and the Salt Bearers proceeded to the summit of the hill; but the wind being boisterous, he could not exhibit his dexterity in displaying his flag, and the space being too small before the carriages, from the concourse of spectators, the King kindly acquiesced in not having it displayed under such inconvenience. \"    0.999915\n",
      "\"That has been a good deal the case, my dear; but not to the degree you mention.                                                                                                                                                                                                                                                                                                                 0.999912\n",
      "Then thou shalt behold Whether by supplication we intend Address, and to begirt the almighty throne Beseeching or besieging.                                                                                                                                                                                                                                                                     0.999911\n",
      "Name: 0, dtype: float64\n",
      "Component 1:\n",
      "text\n",
      "names         0.501064\n",
      "Year          0.501043\n",
      "Others        0.501042\n",
      "Book V        0.501032\n",
      "CHAPTER       0.501030\n",
      "CHAPTER       0.501030\n",
      "wink'd        0.501030\n",
      "CHAPTER IV    0.501027\n",
      "CHAPTER XI    0.501027\n",
      "CHAPTER V     0.501027\n",
      "Name: 1, dtype: float64\n",
      "Component 2:\n",
      "text\n",
      "wafted hither          0.238886\n",
      "May Praise             0.238881\n",
      "_him_.                 0.238877\n",
      "_ _                    0.238873\n",
      "Thou hast              0.238815\n",
      "Hast thou              0.238767\n",
      "art thou               0.238704\n",
      "_dissolved_            0.231807\n",
      "_understand_           0.231807\n",
      "What can be auoyded    0.221567\n",
      "Name: 2, dtype: float64\n",
      "Component 3:\n",
      "text\n",
      "heere                     0.605021\n",
      "Forth                     0.604923\n",
      "Kind                      0.604902\n",
      "Kind                      0.604902\n",
      "So unreasonably early!    0.530986\n",
      "Why not?                  0.499192\n",
      "\"Not quite so well.       0.499090\n",
      "How now?                  0.499087\n",
      "No indeed!                0.498943\n",
      "No more!                  0.498943\n",
      "Name: 3, dtype: float64\n",
      "Component 4:\n",
      "text\n",
      "Mrs. T.           0.229725\n",
      "of Mr.            0.229608\n",
      "Mrs. Weston       0.229601\n",
      "Mr. Talbot        0.229491\n",
      "Mr. Knightley!    0.229183\n",
      "Mrs. Talbot.      0.229096\n",
      "Mrs. Talbot.      0.229096\n",
      "Mrs. Talbot!      0.229096\n",
      "Yes.              0.229089\n",
      "Yes!              0.229089\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#Our SVD data reducer.  We are going to reduce the feature space from 27911 to 10.\n",
    "svd= TruncatedSVD(10)\n",
    "\n",
    "# Train the data for features since there's non tf-idf data in the features section.\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(features)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    "#Looking at what sorts of paragraphs our solution considers similar, for the first five identified topics\n",
    "paras_by_component=pd.DataFrame(X_train_lsa,index=X_train)\n",
    "for i in range(5):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(paras_by_component.loc[:,i].sort_values(ascending=False)[0:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It captured a lot of the variance of the data, perhaps too much. Lets check the sentence similarity. \n",
    "\n",
    "## Sentence Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a27ce0898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key:\n",
      "0 There had been no real affection either in his language or manners.\n",
      "1 The first to speak was Gogol, the irreconcilable, who seemed bursting with inarticulate grievance. \"\n",
      "2 I swear I will never henceforth have to do with the faith that tells the best, I will have to do only with that faith that leaves the best untold.\n",
      "3 `If _Miss_ _Taylor_ undertakes to wrap Miss Emma up, you need not have any fears, sir.'\n",
      "4 If we go anywhere we'll go together to meet what happens,\n",
      "5 That's well done, Wheeler; fight the by-battle there with Bursal.\n",
      "6 See, Antony that Reuels long a-nights Is notwithstanding vp.\n",
      "7 But as for making yourself clear, it is the last thing you do.\n",
      "8 Not that Emma was gay and thoughtless from any real felicity; it was rather because she felt less happy than she had expected.\n",
      "9 Let others dispose of questions, I dispose of nothing, I arouse unanswerable questions, Who are they I see and touch, and what about them?\n"
     ]
    }
   ],
   "source": [
    "# Compute document similarity using LSA components\n",
    "similarity = np.asarray(np.asmatrix(X_train_lsa) * np.asmatrix(X_train_lsa).T)\n",
    "#Only taking the first 10 sentences\n",
    "sim_matrix=pd.DataFrame(similarity,index=X_train).iloc[0:10,0:10]\n",
    "#Making a plot\n",
    "ax = sns.heatmap(sim_matrix,yticklabels=range(10))\n",
    "plt.show()\n",
    "\n",
    "#Generating a key for the plot.\n",
    "print('Key:')\n",
    "for i in range(10):\n",
    "    print(i,sim_matrix.index[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning Classification Models \n",
    "\n",
    "I'm going to attempt the supervised classification models to see how well the different models perform. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set score without clustering:0.48640(+/- 0.011)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the random forest model\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "# Train and fit the model.\n",
    "train = rfc.fit(X2_train, y2_train)\n",
    "\n",
    "rfc_scores = cross_val_score(rfc, X2_train, y2_train, cv=5)\n",
    "\n",
    "print('\\nTraining set score without clustering:{:.5f}(+/- {:.3f})'.format(rfc_scores.mean(), rfc_scores.std()*2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set score without clustering:0.52621(+/- 0.006)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize the model.\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Train the model.\n",
    "train = lr.fit(X2_train, y2_train)\n",
    "\n",
    "# Obtain the cross val score\n",
    "lr_scores = cross_val_score(lr, X2_train, y_train, cv=5)\n",
    "print('\\nTraining set score without clustering:{:.5f}(+/- {:.3f})'.format(lr_scores.mean(), lr_scores.std()*2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set score without clustering:0.54326(+/- 0.012)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clf = GradientBoostingClassifier()\n",
    "train = clf.fit(X2_train, y2_train)\n",
    "clf_scores = cross_val_score(clf, X2_train, y_train, cv=5)\n",
    "print('\\nTraining set score without clustering:{:.5f}(+/- {:.3f})'.format(clf_scores.mean(), clf_scores.std()*2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the hyperparameters\n",
    "\n",
    "Since the gradient boosting model did the best I will try to increase that model's score for the clustering model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-8ba75eeedabc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#fitting model and printing best parameters and score from model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mgrid_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mgrid_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX2_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Best Score:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 639\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Set of parameters to test for best score in Grid Search CV\n",
    "clf_params = {'loss':['deviance'],\n",
    "             'max_depth':[2,4,6],\n",
    "             'max_features':['auto'],\n",
    "             'n_estimators':[50,100,200,500]}\n",
    "\n",
    "#fitting model and printing best parameters and score from model\n",
    "grid_clf = GridSearchCV(clf, clf_params, cv=5, verbose=1, n_jobs=-1)\n",
    "grid_clf.fit(X2_train, y_train)\n",
    "\n",
    "print('Best Score:', grid_clf.best_score_)\n",
    "best_params_clf = grid_clf.best_params_\n",
    "print('Best Parameters:', best_params_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.51656222, 0.50984957, 0.51451093, 0.51119871, 0.51460835])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2 = GradientBoostingClassifier(loss='deviance', max_depth=2,\n",
    "                                 max_features='auto', n_estimators=500)\n",
    "\n",
    "cross_val_score(clf2, X2_train_c, y2_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
